{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This notebook can be run using spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygeohash as pgh\n",
    "from haversine import haversine\n",
    "\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master(\"local\")\n",
    "         .appName(\"my_local\")\n",
    "         .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "drive = spark.read.parquet('drive/')\n",
    "vehicle = spark.read.csv('vehicle.csv', header=True)\n",
    "weather = spark.read.parquet('weather/')\n",
    "trip = spark.read.parquet('trip/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------+-------+-------+-------+-------------------+--------+----------+----+-------+-----------------+\n",
      "|vehicle_id|             trip_id|           datetime|velocity|accel_x|accel_y|accel_z|engine_coolant_temp|eng_load|fuel_level| iat|    rpm|__index_level_0__|\n",
      "+----------+--------------------+-------------------+--------+-------+-------+-------+-------------------+--------+----------+----+-------+-----------------+\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:00|     0.0|  83.12|  72.16|  69.85|               91.0|   210.6|    101.88|82.0|2123.04|                0|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:01|   82.28|  82.08|  75.38|  78.92|               90.0|  209.45|    104.43|80.0|2119.44|                1|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:02|   83.72|  72.93|  71.72|  76.73|               98.0|  217.33|    104.86|63.0|2118.09|                2|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:03|   67.14|  81.24|  75.23|  72.74|               92.0|  221.34|    113.64|72.0|2116.97|                3|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:04|   44.04|  75.48|  73.24|  79.41|               94.0|  207.22|    100.57|70.0|2115.27|                4|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:05|   70.02|   84.0|  80.82|  77.17|               85.0|  212.34|     99.27|67.0|2125.47|                5|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:06|   50.48|  85.02|  76.38|  81.35|               92.0|  217.57|     100.6|72.0|2118.49|                6|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:07|   87.43|  74.26|  69.37|  79.01|               87.0|  211.02|     99.13|73.0|2112.61|                7|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:08|   66.81|  74.03|  80.21|  84.41|              102.0|  217.83|    105.93|75.0|2121.11|                8|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:09|   58.24|  79.38|  77.73|   74.8|               97.0|   204.6|     92.98|74.0|2111.35|                9|\n",
      "+----------+--------------------+-------------------+--------+-------+-------+-------+-------------------+--------+----------+----+-------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drive.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+-------+------------------+--------+-----------------+\n",
      "|vehicle_id|             trip_id|           datetime|    lat|              long|velocity|__index_level_0__|\n",
      "+----------+--------------------+-------------------+-------+------------------+--------+-----------------+\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:00|31.8125|          -83.9375|     0.0|                0|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:01|31.8125|-83.93722222222223|   82.28|                1|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:02|31.8125|-83.93694444444445|   83.72|                2|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:03|31.8125|-83.93666666666667|   67.14|                3|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:04|31.8125|-83.93638888888889|   44.04|                4|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:05|31.8125|-83.93611111111112|   70.02|                5|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:06|31.8125|-83.93583333333333|   50.48|                6|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:07|31.8125|-83.93555555555555|   87.43|                7|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:08|31.8125|-83.93527777777778|   66.81|                8|\n",
      "|   1000516|36417dfecb2049769...|2017-01-08 11:00:09|31.8125|           -83.935|   58.24|                9|\n",
      "+----------+--------------------+-------------------+-------+------------------+--------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----------+-----------+----------+----------+--------------+------------------+--------------+-------------------+---------+------------------+-----------------+--------------------+---------+----------------+-----------------+\n",
      "|vehicle_id|year|      make|      Model|drivetrain|max_torque|max_horsepower|max_horsepower_rpm|max_torque_rpm|engine_displacement|fuel_type|fuel_tank_capacity|fuel_economy_city|fuel_economy_highway|cylinders|forced_induction|device_generation|\n",
      "+----------+----+----------+-----------+----------+----------+--------------+------------------+--------------+-------------------+---------+------------------+-----------------+--------------------+---------+----------------+-----------------+\n",
      "|   1000500|2016|     Honda|      Civic|         2|       174|           140|              6500|          1500|              1.799|     1059|                47|             16.5|                  20|        4|            1054|                5|\n",
      "|   1000501|2016|      Jeep|    Compass|         4|       350|           171|              3750|          1250|              1.956|     1059|                60|             17.1|                21.2|        3|            null|                4|\n",
      "|   1000502|2016|   Hyundai|      Creta|         2|       260|           126|              4000|          1500|              1.582|     1059|                55|            19.67|                24.1|        4|            1054|                3|\n",
      "|   1000503|2016|     Skoda|     Superb|         2|       250|           177|              5100|          1750|              1.798|     1059|                66|            14.67|                23.3|        3|            null|                2|\n",
      "|   1000504|2017|Volkswagen|     Passat|         4|       350|           174|              3600|          1500|              1.968|     1059|                66|            17.42|               20.43|        4|            null|                4|\n",
      "|   1000506|2017|       BMW|3 Series GT|         2|       400|           188|              4000|          1800|              1.995|     1059|                57|            21.76|               26.32|        3|            null|                3|\n",
      "|   1000507|2017|      Audi|         Q3|         3|       250|           148|              5000|          1400|              1.395|     1059|                64|             16.9|                21.4|        3|            1054|                2|\n",
      "+----------+----+----------+-----------+----------+----------+--------------+------------------+--------------+-------------------+---------+------------------+-----------------+--------------------+---------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vehicle.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question1 - engine_features.csv\n",
    "\n",
    "join drive and vehicles df\n",
    "vehicles file is super small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive2 = drive.select('vehicle_id','datetime','eng_load','rpm')\n",
    "drive2 = drive2.withColumn('vehicle_id', drive2.vehicle_id.cast('string'))\n",
    "\n",
    "vehicle2 = vehicle.select('vehicle_id','max_torque','max_horsepower','max_horsepower_rpm')\n",
    "\n",
    "drive2 = drive2.join(vehicle2, on='vehicle_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive2 = drive2.withColumn('pst_timestamp', F.from_utc_timestamp('datetime', 'PST'))\n",
    "drive2 = drive2.drop('datetime')\n",
    "drive2 = drive2.na.fill({'max_torque':0, 'max_horsepower': 0, 'max_horsepower_rpm': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive2 = (drive2\n",
    "          .withColumn('active_horsepower', \n",
    "                      ((F.col('eng_load') / 255) * (F.col('max_torque') * F.col('rpm'))) / 5252 ))\n",
    "\n",
    "drive2 = (drive2\n",
    "          .withColumn('horsepower_util', (F.col('active_horsepower') / F.col('max_horsepower'))))\n",
    "\n",
    "drive2 = (drive2\n",
    "          .withColumn('torque_util', (F.col('eng_load') / 255)))\n",
    "\n",
    "drive2 = (drive2\n",
    "          .withColumn('rpm_util', (F.col('rpm') / F.col('max_horsepower_rpm'))))\n",
    "\n",
    "drive2 = drive2.na.fill({'horsepower_util':0, 'rpm_util': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"vehicle_id\", StringType()),\n",
    "    StructField(\"week_start_date\", TimestampType()),\n",
    "    StructField(\"ft_torque_util_60pct_s\", DoubleType()),\n",
    "    StructField(\"ft_torque_util_70pct_s\", DoubleType()),\n",
    "    StructField(\"ft_torque_util_80pct_s\", DoubleType()),\n",
    "    StructField(\"ft_torque_util_90pct_s\", DoubleType()),\n",
    "    StructField(\"ft_horsepower_util_50pct_s\", DoubleType()),\n",
    "    StructField(\"ft_horsepower_util_60pct_s\", DoubleType()),\n",
    "    StructField(\"ft_horsepower_util_70pct_s\", DoubleType()),\n",
    "    StructField(\"ft_horsepower_util_80pct_s\", DoubleType()),\n",
    "    StructField(\"ft_rpm_util_50pct_s\", DoubleType()),\n",
    "    StructField(\"ft_rpm_util_60pct_s\", DoubleType())\n",
    "])\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def calculate(df):\n",
    "    \n",
    "    def calculate_pct_secs(pdf, feat, pct1, pct2):\n",
    "        \n",
    "        # total feat value\n",
    "        total_torque = pdf[feat].sum() \n",
    "        \n",
    "        # % of total torque\n",
    "        first_pct = pct1*total_torque\n",
    "        second_pct = pct2*total_torque\n",
    "        \n",
    "        # cumsum\n",
    "        pdf['time_cumsum'] = pdf[feat].cumsum()\n",
    "        pdf = pdf.loc[(pdf['time_cumsum'] >= first_pct) & (pdf['time_cumsum'] < second_pct)]\n",
    "        secs = (pdf['pst_timestamp'].max() - pdf['pst_timestamp'].min()) / pd.to_timedelta(1, unit='S')\n",
    "        return secs\n",
    "   \n",
    "    \n",
    "    def wrapper(pdf):\n",
    "        \n",
    "        sol = {}\n",
    "        pdf['pst_timestamp'] = pd.to_datetime(pdf['pst_timestamp'])\n",
    "        \n",
    "        vals = {'torque_util': [('ft_torque_util_60pct_s',0.60,0.70),\n",
    "                                ('ft_torque_util_70pct_s',0.70,0.80),\n",
    "                                ('ft_torque_util_80pct_s',0.80,0.90),\n",
    "                                ('ft_torque_util_90pct_s',0.90,1.0)],\n",
    "               'horsepower_util' : [('ft_horsepower_util_50pct_s',0.50,0.60),\n",
    "                                    ('ft_horsepower_util_60pct_s',0.60,0.70),\n",
    "                                    ('ft_horsepower_util_70pct_s',0.70,0.80),\n",
    "                                    ('ft_horsepower_util_80pct_s',0.80,0.90)],\n",
    "               'rpm_util': [('ft_rpm_util_50pct_s',0.50,0.60),\n",
    "                            ('ft_rpm_util_60pct_s',0.60,0.70)]}\n",
    "        \n",
    "        for feat, val in vals.items():\n",
    "    \n",
    "            for new_feat, pct1, pct2 in val:\n",
    "                # print('inside list')\n",
    "                p = calculate_pct_secs(pdf, feat, pct1, pct2)\n",
    "                sol[new_feat] = p if p > 0 else 0\n",
    "                print(sol)\n",
    "            \n",
    "        return pd.Series(sol)\n",
    "    \n",
    "    p = df.groupby(['vehicle_id', pd.Grouper(key='pst_timestamp', freq='W-MON')]).apply(wrapper).reset_index()\n",
    "    print(p.columns)\n",
    "    print(p.head())\n",
    "    p.columns = ['vehicle_id',\n",
    "                 'week_start_date',\n",
    "                 'ft_torque_util_60pct_s',\n",
    "                 'ft_torque_util_70pct_s',\n",
    "                 'ft_torque_util_80pct_s',\n",
    "                 'ft_torque_util_90pct_s',\n",
    "                 'ft_horsepower_util_50pct_s',\n",
    "                 'ft_horsepower_util_60pct_s',\n",
    "                 'ft_horsepower_util_70pct_s',\n",
    "                 'ft_horsepower_util_80pct_s',\n",
    "                 'ft_rpm_util_50pct_s',\n",
    "                 'ft_rpm_util_60pct_s']\n",
    "    return p\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = drive2.groupby('vehicle_id').apply(calculate)\n",
    "g = g.withColumn('week_start_date', F.date_format('week_start_date', 'YYYY-MM-DD'))\n",
    "g = g.orderBy('vehicle_id','week_start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g.repartition(1).write.csv('./engine_features', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr engine_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. drive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = drive.select('trip_id','datetime','velocity')\n",
    "g2 = g2.withColumn('lag_velocity', F.lag(g2['velocity']).over(Window.partitionBy('trip_id').orderBy('trip_id')))\n",
    "g2 = g2.withColumn('diff', F.col('velocity') - F.col('lag_velocity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"trip_id\", StringType()),\n",
    "    StructField(\"ft_cnt_vehicle_deaccel_val\", IntegerType()),\n",
    "    StructField(\"ft_sum_hard_brakes_10_flg_val\", IntegerType()),\n",
    "    StructField(\"ft_sum_hard_brakes_3_flg_val\", IntegerType()),\n",
    "    StructField(\"ft_sum_time_deaccel_val\", IntegerType()),\n",
    "    StructField(\"ft_cnt_vehicle_accel_val\", IntegerType()),\n",
    "    StructField(\"ft_sum_hard_accel_10_flg_val\", IntegerType()),\n",
    "    StructField(\"ft_sum_hard_accel_3_flg_val\", IntegerType()),\n",
    "    StructField(\"ft_sum_time_accel_val\", IntegerType())\n",
    "    #StructField(\"velocity\", DoubleType()),\n",
    "    #StructField(\"lag_velocity\", DoubleType()),\n",
    "    #StructField(\"diff\", DoubleType()),\n",
    "   \n",
    "])\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def compute(df):\n",
    "    \n",
    "    def inner_compute(df):\n",
    "        from itertools import groupby, zip_longest\n",
    "\n",
    "        x = (list(v) for k,v in groupby(df['diff'], lambda x: x < 0))\n",
    "        l = list(zip_longest(x, x, fillvalue=[]))\n",
    "        l = [i for s in l for i in s]\n",
    "\n",
    "        # count of deaccelaration\n",
    "        cou_de = sum([1 for i in l if sum(i) > 0])\n",
    "        \n",
    "        # deacc < -10\n",
    "        cou_le = sum([1 for i in l if any([y for y in i if y <= -10])])\n",
    "        \n",
    "        # deacc <= -3 and > -10\n",
    "        cou_pe = sum([1 for i in l if any([y for y in i if y <= -3 and y > -10])])\n",
    "        \n",
    "        # seconds, time taken in deaccelaration - assuming each tracking is 1 sec\n",
    "        time_taken = sum(1 for x in pd.np.concatenate(l) if x < 0)\n",
    "        \n",
    "        # accelaration count\n",
    "        acc_count = sum([1 for i in l if any([y for y in i if y > 0])])\n",
    "        \n",
    "        # accelaration count > 10\n",
    "        acc_count_10 = sum([1 for i in l if any([y for y in i if y >= 10])])\n",
    "        \n",
    "        # accelaration count > 3 and count < 10\n",
    "        acc_count_3 = sum([1 for i in l if any([y for y in i if y >= 3 and y < 10])])\n",
    "        \n",
    "        # time taken accel\n",
    "        time_taken_accel = sum(1 for x in pd.np.concatenate(l) if x > 0)\n",
    "        \n",
    "        \n",
    "        return pd.Series({'ft_cnt_vehicle_deaccel_val': cou_de, \n",
    "                          'ft_sum_hard_brakes_10_flg_val': cou_le,\n",
    "                          'ft_sum_hard_brakes_3_flg_val': cou_pe,\n",
    "                          'ft_sum_time_deaccel_val': time_taken,\n",
    "                          'ft_cnt_vehicle_accel_val': acc_count,\n",
    "                          'ft_sum_hard_accel_10_flg_val': acc_count_10,\n",
    "                          'ft_sum_hard_accel_3_flg_val': acc_count_3,\n",
    "                          'ft_sum_time_accel_val': time_taken_accel})\n",
    "    \n",
    "    p = df.groupby('trip_id').apply(inner_compute).reset_index()\n",
    "    #p.columns = ['trip_id',\n",
    "    #             'ft_cnt_vehicle_deaccel_val',\n",
    "    #             'ft_sum_hard_brakes_10_flg_val']\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g3 = g2.groupby('trip_id').apply(compute).orderBy('trip_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g3.repartition(1).write.csv('./drive_features', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr drive_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = spark.read.parquet('weather/')\n",
    "trip = spark.read.parquet('trip/')\n",
    "trip = trip.drop('__index_level_0__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip = trip.withColumn('long', F.format_number('long', 4))\n",
    "trip = trip.withColumn('long', F.col('long').cast('double'))\n",
    "trip = trip.withColumnRenamed('long','lon')\n",
    "\n",
    "trip = trip.withColumn('lat', F.format_number('lat', 4))\n",
    "trip = trip.withColumn('lat', F.col('lat').cast('double'))\n",
    "\n",
    "trip = trip.withColumn('pst_timestamp', F.from_utc_timestamp('datetime', 'PST'))\n",
    "trip = trip.withColumn('time', F.hour('pst_timestamp'))\n",
    "trip = trip.withColumn('date', F.to_date('pst_timestamp', 'YYYY-MM-DD'))\n",
    "trip = trip.withColumn('date', F.col('date').cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.withColumn('time', F.regexp_replace('time', r'[^\\d+]',''))\n",
    "weather = weather.withColumn('time', F.col('time').cast('integer'))\n",
    "weather = weather.withColumn('date', F.col('date').cast('string'))\n",
    "\n",
    "weather = weather.withColumn('temperature_data', F.format_number('temperature_data', 2))\n",
    "weather = weather.withColumn('temperature_data', F.col('temperature_data').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode lat. long\n",
    "udf1 = F.udf(lambda x, y: pgh.encode(x,y,precision=5))\n",
    "trip = trip.withColumn('encoded_ll',udf1('lat','lon'))\n",
    "weather = weather.withColumn('encoded_ll',udf1('lat','lon'))\n",
    "\n",
    "# convert kelvin to farenheit\n",
    "# (296K − 273.15) × 9/5 + 32 = 73.13°F\n",
    "udf2 = F.udf(lambda x: round(((x - 273.15) * (9/5) + 32), 2))\n",
    "weather = weather.withColumn('temperature_data', udf2('temperature_data'))\n",
    "weather = weather.withColumn('temperature_data', F.col('temperature_data').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip = trip.withColumnRenamed('lon','trip_long')\n",
    "trip = trip.withColumnRenamed('lat','trip_lat')\n",
    "\n",
    "weather = weather.withColumnRenamed('lon','weather_long')\n",
    "weather = weather.withColumnRenamed('lat','weather_lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip2 = trip.select('vehicle_id','pst_timestamp','date','time','trip_lat','trip_long','encoded_ll')\n",
    "trip2 = trip2.join(weather, on=['date','time','encoded_ll'], how='left') #.orderBy('date','time','encoded_ll')\n",
    "trip2 = trip2.where('x is not null')\n",
    "trip2 = trip2.withColumn('vehicle_id', F.col('vehicle_id').cast('string'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_weather(mx):\n",
    "    \n",
    "    if mx < 27:\n",
    "        return 'SNOW'\n",
    "    elif mx >= 27 and mx < 32:\n",
    "        return 'FREEZING RAIN'\n",
    "    else:\n",
    "        return 'RAIN'\n",
    "\n",
    "udf3 = F.udf(lambda x: map_weather(x))\n",
    "trip2 = trip2.withColumn('weather_label', udf3('temperature_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weather perception\n",
    "def map_per(mx):\n",
    "    \n",
    "    if mx < 2.5:\n",
    "        return 'LIGHT'\n",
    "    elif mx >= 2.5 and mx < 7.6:\n",
    "        return 'MODERATE'\n",
    "    else:\n",
    "        return 'HEAVY'\n",
    "\n",
    "udf4 = F.udf(lambda x: map_per(x))\n",
    "trip2 = trip2.withColumn('perception_label', udf4('precipitation_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"vehicle_id\", StringType()),\n",
    "    StructField(\"pst_timestamp\", TimestampType()),\n",
    "    StructField(\"total_light_rain_driving_km\", IntegerType()),\n",
    "    StructField(\"total_light_freezing_rain_driving_km\", IntegerType()),\n",
    "    StructField(\"total_light_snow_driving_km\", IntegerType()),\n",
    "    StructField(\"total_moderate_rain_driving_km\", IntegerType()),\n",
    "    StructField(\"total_moderate_freezing_rain_driving_km\", IntegerType()),\n",
    "    StructField(\"total_moderate_snow_driving_km\", IntegerType()),\n",
    "    StructField(\"total_heavy_rain_driving_km\", IntegerType()),    \n",
    "])\n",
    "\n",
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def compute(pdf):\n",
    "    \n",
    "    def compute_haversine(combs):\n",
    "        val = 0\n",
    "        prev = combs[0]\n",
    "        \n",
    "        for i in combs[1:]:\n",
    "            g = haversine(i, prev, unit='km')\n",
    "            val += abs(g)\n",
    "            prev = i\n",
    "        \n",
    "        return val\n",
    "        \n",
    "    \n",
    "    def inner_compute(pdf):\n",
    "        \n",
    "        # create data fraames\n",
    "        ft1 = pdf[(pdf['perception_label'] == 'LIGHT') & (pdf['weather_label'] == 'RAIN')]\n",
    "        ft2 = pdf[(pdf['perception_label'] == 'LIGHT') & (pdf['weather_label'] == 'FREEZING RAIN')]\n",
    "        ft3 = pdf[(pdf['perception_label'] == 'LIGHT') & (pdf['weather_label'] == 'SNOW')]\n",
    "        ft4 = pdf[(pdf['perception_label'] == 'MODERATE') & (pdf['weather_label'] == 'RAIN')]\n",
    "        ft5 = pdf[(pdf['perception_label'] == 'MODERATE') & (pdf['weather_label'] == 'FREEZING RAIN')]\n",
    "        ft6 = pdf[(pdf['perception_label'] == 'MODERATE') & (pdf['weather_label'] == 'SNOW')]\n",
    "        ft7 = pdf[(pdf['perception_label'] == 'HEAVY') & (pdf['weather_label'] == 'RAIN')]\n",
    "        \n",
    "        if ft1.shape[0] != 0:\n",
    "            combs1 = ft1[['trip_lat','trip_long']].apply(tuple, axis=1).tolist()\n",
    "            # print(combs[:4])\n",
    "            dist_light_rain = compute_haversine(combs1)\n",
    "        else:\n",
    "            dist_light_rain = 0\n",
    "        \n",
    "        if ft2.shape[0] != 0:\n",
    "            combs2 = ft2[['trip_lat','trip_long']].apply(tuple, axis=1).tolist()\n",
    "            # print(combs[:4])\n",
    "            dist_light_freezing_rain = compute_haversine(combs2)\n",
    "        else:\n",
    "            dist_light_freezing_rain = 0  \n",
    "        \n",
    "        \n",
    "        if ft3.shape[0] != 0:\n",
    "            combs3 = ft3[['trip_lat','trip_long']].apply(tuple, axis=1).tolist()\n",
    "            # print(combs[:4])\n",
    "            dist_light_snow = compute_haversine(combs3)\n",
    " \n",
    "        else:\n",
    "            dist_light_snow = 0  \n",
    "        \n",
    "        if ft4.shape[0] != 0:\n",
    "            combs4 = ft4[['trip_lat','trip_long']].apply(tuple, axis=1).tolist()\n",
    "            # print(combs[:4])\n",
    "            dist_rain_snow = compute_haversine(combs4)\n",
    " \n",
    "        else:\n",
    "            dist_rain_snow = 0\n",
    "        \n",
    "        if ft5.shape[0] != 0:\n",
    "            combs5 = ft5[['trip_lat','trip_long']].apply(tuple, axis=1).tolist()\n",
    "            # print(combs[:4])\n",
    "            dist_freezing_rain_snow = compute_haversine(combs5)\n",
    " \n",
    "        else:\n",
    "            dist_freezing_rain_snow = 0\n",
    "        \n",
    "        if ft6.shape[0] != 0:\n",
    "            combs6 = ft6[['trip_lat','trip_long']].apply(tuple, axis=1).tolist()\n",
    "            # print(combs[:4])\n",
    "            dist_moderate_rain_snow = compute_haversine(combs6)\n",
    " \n",
    "        else:\n",
    "            dist_moderate_rain_snow = 0\n",
    "            \n",
    "        \n",
    "        if ft7.shape[0] != 0:\n",
    "            combs7 = ft7[['trip_lat','trip_long']].apply(tuple, axis=1).tolist()\n",
    "            # print(combs[:4])\n",
    "            dist_heavy_rain_driving = compute_haversine(combs7)\n",
    " \n",
    "        else:\n",
    "            dist_heavy_rain_driving = 0\n",
    "    \n",
    "        \n",
    "        return pd.Series({'total_light_rain_driving_km' : round(dist_light_rain),\n",
    "                          'total_light_freezing_rain_driving_km' : round(dist_light_freezing_rain),\n",
    "                           'total_light_snow_driving_km': round(dist_light_snow),\n",
    "                           'total_moderate_rain_driving_km': round(dist_rain_snow) ,\n",
    "                           'total_moderate_freezing_rain_driving_km': round(dist_freezing_rain_snow),\n",
    "                           'total_moderate_snow_driving_km': round(dist_moderate_rain_snow),\n",
    "                           'total_heavy_rain_driving_km': round(dist_heavy_rain_driving)})\n",
    "    \n",
    "    df = pdf.groupby(['vehicle_id', pd.Grouper(key='pst_timestamp', freq='W-MON')]).apply(inner_compute).reset_index()\n",
    "    print(df.columns)\n",
    "    print(df.head())\n",
    "    # df.columns = ['vehicle_id','Week_start_date','total_light_rain_driving_km']\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trip3 = trip2.groupby('vehicle_id').apply(compute)\n",
    "\n",
    "trip3 = trip3.withColumnRenamed('pst_timestamp', 'week_start_date')\n",
    "\n",
    "trip3 = trip3.withColumn('week_start_date', F.date_format('week_start_date', 'YYYY-MM-DD'))\n",
    "\n",
    "trip3 = trip3.orderBy('vehicle_id','week_start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip3.repartition(1).write.csv('./weather_features', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr weather_features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
